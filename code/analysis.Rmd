---
title: "Particle Interception as a Function of Reynolds Number and Collector Density"
subtitle: "Version 2.0"
author: "Jordan Wingenroth"
date: "8/28/2020"
toc: TRUE
output:
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, fig.pos = "!H", out.extra = "")


```

**Objective(s)**

In the interest of documenting our analytic methods, I've decided to make one comprehensive, hopefully lay-readable document including all steps from our raw laboratory data to the model results. I won't be able to include the turbulence analyses in time for our meeting (8/27), but that required Laurel's Matlab script anyways.

## Data Tidying

**CHANGES FROM VERSION 1 (8/26)**

I pared back this section to reduce the page count. Please refer to the original version to see visualizations and more detailed steps and notes.

**Time-decay runs:**

We entered data into Google Sheets as we processed samples in the lab. I downloaded .csv versions of these data in 2019, then made a few modifications as I explored confounders. Today, I added data from runs done in my absence to this dataset. Field names, date syntax, and other conventions weren't kept consistent, so some data wrangling is necessary. I also removed the blank data at the beginning of some files manually.

```{r, message=FALSE, warning=FALSE}

library(tidyverse)

```


```{r}

pumpfiles <- list.files("../data/peristaltic pumps/")
trapfiles <- list.files("../data/sediment traps/")

```

```{r}

pumpdate <- str_sub(pumpfiles,0,6)
trapdate <- str_sub(trapfiles,0,6)

```

In all, five runs are missing sediment trap data, meaning they won't be used in our final, published analyses.

Let's turn our attention to the suspended concentration data (i.e., pump data).

```{r, message=FALSE, warning=FALSE}

pumpdata <- lapply(pumpfiles, function(x) read_csv(paste0("../data/peristaltic pumps/",x)))

names(pumpdata) <- pumpdate

x <- pumpdata

tidypump <- lapply(seq_along(x), function(i) {
  select(x[[i]], 
         loc = Location, 
         ht = Height,
         t = `time series`,
         mvc = contains("(ppm)")) %>%
    filter(t < 21) %>% #filter a few timepoints outside the normal window 
    mutate(t = (t-min(t)+1)*300, #convert from timestep to seconds
           mvc = as.numeric(mvc), 
           date = as.numeric(names(x)[[i]])) %>%
    filter(mvc<80, mvc>8) #outliers were removed based on the residual graph
  }
  )

pump <- bind_rows(tidypump)

```

So now we have a long table of our pump data (time $\times$ concentration) stratified by run (i.e., treatment), height, and upstream/downstream location.

Aside from a few runs where **part** or **all** of the data don't follow the decay pattern, the results look pretty good! If we repeated runs as necessary to replace those with erroneous data, that shouldn't be a problem. Here I'll narrow down our dataset by knocking out runs with clear issues.

```{r}

pump <- pump %>%

#first 3 runs had starting sediment mass of 100g rather than 200g

    filter(date > 181005) %>%
  
#we can't use runs without sediment mass
  
    filter(as.character(date) %in% trapdate)
  
```

So this leaves 20 rows. Let's join this to our run metadata table.

```{r message=FALSE}

metadata <- read_csv("../data/run_metadata.csv")

pump <- left_join(pump, metadata, by = "date")

```

Now then, let's remove the biofouled runs and a couple runs with other issues, which won't be used in assessing our primary hypothesis about Reynolds number and collector density, and we'll see what we're left with.

```{r}

pumpfinal <- pump %>%
  filter(growth_days==0, date != 190417, dowel_density != "0232")

```

There are certainly no trends jumping out immediately, which isn't necessarily worrisome since exponential decay is hard to compare by eye. More on that later though. 

For the sediment traps, data import and wrangling follows a similar structure to the pump data.

```{r, message=FALSE, warning=FALSE}

trapdata <- lapply(trapfiles, function(x) read_csv(paste0("../data/sediment traps/",x)))

names(trapdata) <- trapdate

x <- trapdata

tidytrap <- lapply(seq_along(x), function(i) {
  select(x[[i]], station = 1, pre = contains("pre"), post = contains("post"), sed = contains("sed")) %>%
  mutate(date = names(x)[i]) %>%
  mutate_at(vars(pre,post,sed,date), as.numeric)
  }
  )

trap <- bind_rows(tidytrap) %>%
  filter(!is.na(sed))

```

So, across 21 runs, we had 186 samples. We were set up to collect 9 per run, but traps broke, filters slipped and spilled sediment, et cetera. $186/21 =$ `r 186/21`, so our success rate was actually pretty good, although some of those measurements were probably erroneous despite being measured. 

```{r}

trapfinal <- left_join(trap, metadata, by = "date") %>%
  filter(date %in% pumpfinal$date)

```

**Velocity experiment:**

We estimated Reynolds number from a flow velocity experiment in the open channel conducted on Jan 28, 2019. The Vectrino data is in commit `e78357` and the R code for the regression in commit `17c8af` in the esdlflume Github repo (Thanks Candace!).

There's an image in version 1 (and in /pics) but the linear regression is simply $u = 0.00081282 + 0.00195723f$, where $u$ is flow velocity (m/s) and $f$ is pump frequency (Hz). Hence our velocities at 10, 20, and 30 Hz, were `r 0.00081282 + 0.00195723*10`, `r 0.00081282 + 0.00195723*20`, and `r 0.00081282 + 0.00195723*30` respectively.

It should be noted that the back-of-envelope estimate of $v = f / 500$ is very close for our range, probably by design. For the regression, $R^2 > .99$.

**Flume volume experiment:**

The test section is modeled as a rectangular prism, making calculation of volume trivial. However, the water in the flume is in an irregular form. In order to correct for the time water spends flowing outside the test section, where collectors are not acting on particles, we must estimate the total volume of water. We did so using a simple integration of the volumetric flow rate at the drain. We ran an experiment previously in 2018 or 2019, but my physics common sense lapsed: We picked the drain hose up off the ground to take measurements, then stuck it back down in the drain for most of the flow time. This led to an underestimate of about 5-10%. I re-did the analysis. 1st through 4th degree polynomial regression all yield the same volume +/- <0.5% (code in this repo, under /code)

I rounded off to 2.43 $\text m^3$. The test section has been measured as $1.95 \times .6 \times .4 =$ `r 1.95*.6*.4` $\text m^3$. Hence the corrective factor is `r 2.43/0.468`.

## Modelling for ECE%

So at this point, we really only need `pumpfinal`, `trapfinal`, maybe `metadata` for good measure, the frequency-to-velocity regression coefficients, and individual constants about the physical setup such as dowel diameter, flume volume, starting sediment mass, dynamic viscosity etc.

I'll also mention here that our model is based on the general equation: $$k_{tot} = k_s + k_c + k_{bg}$$ where the k's represent total decay rate and portions of it due to settling, collection, and (background) settling in the rest of the flume outside the test section, respectively.

### NEW ADDITION: Uncertainty propogation

Here is a function found on StackExchange.com, which performs error propogation in R using derivatives. Error values should be named "dx", with x being the corresponding variable name.

```{r}

mutate_with_error = function(.data, f) {
  exprs = list(
    # expression to compute new variable values
    deparse(f[[3]]),
    
    # expression to compute new variable errors
    sapply(all.vars(f[[3]]), function(v) {
      dfdp = deparse(D(f[[3]], v))
      sprintf('(d%s*(%s))^2', v, dfdp)
    }) %>%
      paste(collapse='+') %>%
      sprintf('sqrt(%s)', .)
  )
  names(exprs) = c(
    deparse(f[[2]]),
    sprintf('d%s', deparse(f[[2]]))
  )
  
  .data %>%
    # the standard evaluation alternative of mutate()
    mutate_(.dots=exprs)
}

```


### k_tot (total time-decay rate)

We estimate `k_tot` from our pump data by linear regression of `log(mvc)`:

```{r, warning=FALSE, message=FALSE}

library(lme4)

fits <- lmList(data = pumpfinal, log(mvc)~t | date)

summary(fits)

```

The `t` coeffient estimates are the `k_tot` values in units $\text s^{-1}$, no transformation needed aside from a sign change ($\log(\bar{\phi})=\log(\phi_0e^{-kt}) = log(\phi_0)-kt$).

All the t values are high, though the coefficient t-values are relative to 0, meaning they're really just expressing that we're very certain we did indeed add sediment (phew!). But let's see what the fits look like with the data:

```{r}

cbind(pumpfinal, pred = predict(fits)) %>%
  ggplot() +
  geom_point(aes(x = t, y = mvc), size = .75, alpha = .5) +
  geom_line(aes(x = t, y = exp(pred)), color = "red") +
  facet_grid(pump_freq~dowel_density) +
  scale_x_continuous(breaks = c(0,3000,6000), limits = c(0,6000))
 

```

As I noticed back in early 2019, these curves seem to hint at a U-shaped pattern in the residuals.

```{r}

cbind(pumpfinal, resid = residuals(fits)) %>%
  ggplot(aes(x = t, y = resid)) +
  geom_point() +
  geom_smooth()

```

I think this is simply an artifact of nonlinear processes not accounted for in the exponential model. As an example of what I mean, let's imagine there were a hidden variable called "stickiness" differentiating walnut shell particles from one another. If by way of accellerated settling due to floculation, or accellerated collection, those particles were disproportionately removed from suspension early in the timespan of the experiment, the average physical properties of the suspended sediment would change over its course. It also seems likely to me that as sediment adheres to collectors, their efficiency decreases on account of their having less open surface area.

Whatever the explanation is, we (and other researchers using the exponential model) assume it is uniform across our independent variables. I don't know how safe that assumption is, but that'll have to wait for a future paper. It might be testable from our data but `n = 12` seems a bit of a small sample. Then again, we've probably done more like `n = 100` runs over the entire history of the project.

Aside from that U-curve, the fit seems reasonably good. **This residual plot was used to remove the most obvious outliers**, though those in the bottom right (say, those satisfying $(y + 0.4) < (0.1/1000)(x-3500)$) might reasonably be considered outliers too. We might be in the gray area in terms of data-massaging at that point. Any formal outlier removal included in the manuscript will have to follow a *de rigueur*, citable method of course.

So then, k_tot as a function of our independent vars:

```{r}

fitdata <- coefficients(fits) %>%
  mutate(k_t = -t) %>%
  cbind(dk_t = summary(fits)[[4]][,,'t'][,2], date = as.numeric(row.names(coefficients(fits)))) %>%
  left_join(metadata, by = "date")

fitdata %>%
  ggplot(aes(x = pump_freq, y = k_t, ymin = k_t - 1.96*dk_t, ymax = k_t + 1.96*dk_t, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("black","blue","green","red")) +
  theme_minimal()
  
```

Mixed feelings. On the one hand it's reassuring that aside from the switcharoo at 20 Hz, all runs with dowels had greater k_t than runs without. However, it seems like there's some serious variance. **What confounders could we be missing???**

### k_s (time-decay due to settling)

$$ m_s = \frac{k_s}{k_{tot}}(1-e^{-kT})m_0 $$

Or, expressed for $k_s$, $$k_s = \frac{m_s}{m_0}(\frac{1}{1-e^{-kT}})k_{tot}$$

where $m_s$ is mass settled in the test section (estimated from sediment traps), $m_0$ is starting sediment mass (200 g for this study), and $T$ is the total exposure time of the sediment traps (100 min, i.e., 6000 s, for this study).

```{r}

final <- trapfinal %>%
  group_by(date) %>%
  summarise(m_trap = mean(sed)/1000,
            dm_trap = sd(sed)/1000) %>% #average sediment in trap converted from mg to g
  mutate_with_error(m_s ~ m_trap*1.95*.6/(3.14159*.0127^2)) %>% #*A_TS/a_trap
  left_join(fitdata, by = "date") %>%
  mutate_with_error(c_s ~ m_s/200/(1-exp(-k_t*6000))) %>% #intermediate step to squash error 
  mutate_with_error(k_s ~ c_s*k_t)

final %>%
  ggplot(aes(x = pump_freq, y = k_s, ymin = k_s - 1.96*dk_s, ymax = k_s + 1.96*dk_s, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("black","blue","green","red")) +
  theme_minimal()
  
```

ettling across the dowel treatments does seem to decrease, with a less clear pattern across flow velocity.

### k_bg (time-decay due to settling in the rest of the flume)

In control runs, $k_c = 0$, so we can estimate the decay rate in the rest of the flume as $k_{bg} = k_{tot}-k_s$

```{r}

bgvals <- final %>%
  filter(dowel_density=="0000") %>%
  mutate_with_error(k_bg ~ k_t - k_s) %>%
  select(pump_freq, k_bg, dk_bg) %>%
  arrange(pump_freq)

bgvals
  
```

To add it to the table:

```{r}

final <- left_join(final, bgvals, by = "pump_freq")

```

### k_c (time-decay due to collection)

So, now that we have our values in the table, k_c is just a subtraction away:

```{r}

final <- final %>%
  mutate_with_error(k_c ~ k_t - k_s - k_bg)

final %>%
  filter(dowel_density != "0000") %>%
  ggplot(aes(x = pump_freq, y = k_c, ymin = k_c - 1.96*dk_c, ymax = k_c + 1.96*dk_c, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("blue","green","red")) +
  theme_minimal()
```

Unlike the calculation with the measured values at the 20 Hz control, all k_c are in the green, but the pattern is still quite odd.

### eta

```{r}

#temp in flume measured at 22.2C with a calibrated thermometer
#plugged into https://www.engineeringtoolbox.com/water-dynamic-kinematic-viscosity-d_596.html
visc = 9.509e-7 #kinematic viscosity, m2/s

d = .003175 #dowel diameter

final <- final %>%
  mutate(frontal_area = as.numeric(dowel_density)*.003175, 
            u = as.numeric(pump_freq)/500, #velocity 
            Re = u*.003175/visc, #Reynolds #
            k_t = -t, 
            k_c, k_s, k_bg) %>%
  mutate(eta = k_c/u/frontal_area,
         deta = dk_c/u/frontal_area) %>%
  mutate(eta = eta * 2.43/(1.95*.4*.6),
         deta = deta * 2.43/(1.95*.4*.6)) # don't forget to correct for time out of test section!

final %>%
  filter(dowel_density != "0000") %>%
  ggplot(aes(x = pump_freq, y = eta, ymin = eta - 1.96*deta, ymax = eta + 1.96*deta, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("blue","green","red")) +
  theme_minimal()

```

For strictly exploratory purposes, I'll run it again with the problematic 20 Hz control's $k_{tot}$ and $m_s$ values and error bars replaced with the average of the 10 and 30 Hz ones. This is the best guess we have at what we'll get if we redo the run. Though if we redo the control and get values similar to our previous ones, that would be strange but not strictly unpredicted.

```{r}

final[final$date=="190802","k_t"] <- (final[final$date=="190321","k_t"] + final[final$date=="190729","k_t"])/2
final[final$date=="190802","dk_t"] <- (final[final$date=="190321","dk_t"] + final[final$date=="190729","dk_t"])/2

final[final$date=="190802","m_s"] <- (final[final$date=="190321","m_s"] + final[final$date=="190729","m_s"])/2
final[final$date=="190802","dm_s"] <- (final[final$date=="190321","dm_s"] + final[final$date=="190729","dm_s"])/2


final <- final %>%
  mutate_with_error(c_s ~ m_s/200/(1-exp(-k_t*6000))) %>% #intermediate step to squash error 
  mutate_with_error(k_s ~ c_s*k_t) %>%
  select(-k_bg, -dk_bg)

bgvals <- final %>%
  filter(dowel_density=="0000") %>%
  mutate_with_error(k_bg ~ k_t - k_s) %>%
  select(pump_freq, k_bg, dk_bg) %>%
  arrange(pump_freq)

bgvals

final <- left_join(final, bgvals, by = "pump_freq")

final %>%  
  ggplot(aes(x = pump_freq, y = k_s, ymin = k_s - 1.96*dk_s, ymax = k_s + 1.96*dk_s, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("black","blue","green","red")) +
  theme_minimal()

final <- final %>%
  mutate_with_error(k_c ~ k_t - k_s - k_bg)

final %>%
  filter(dowel_density != "0000") %>%
  ggplot(aes(x = pump_freq, y = k_c, ymin = k_c - 1.96*dk_c, ymax = k_c + 1.96*dk_c, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("blue","green","red")) +
  theme_minimal()

final <- final %>%
  mutate(frontal_area = as.numeric(dowel_density)*.003175, 
            u = as.numeric(pump_freq)/500, #velocity 
            Re = u*.003175/visc, #Reynolds #
            k_t = -t, 
            k_c, k_s, k_bg) %>%
  mutate(eta = k_c/u/frontal_area,
         deta = dk_c/u/frontal_area) %>%
  mutate(eta = eta * 2.43/(1.95*.4*.6),
         deta = deta * 2.43/(1.95*.4*.6)) # don't forget to correct for time out of test section!

final %>%
  filter(dowel_density != "0000") %>%
  ggplot(aes(x = pump_freq, y = eta, ymin = eta - 1.96*deta, ymax = eta + 1.96*deta, color = dowel_density)) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 3)) +
  geom_errorbar(width = 3, position = "dodge") +
  scale_color_manual(values = c("blue","green","red")) +
  theme_minimal()


```


## Possible additions

- Turbulence data
    - The calculations are done 
    - But it might be good to include here for the sake of comprehensiveness

