---
title: "Particle Interception as a Function of Reynolds Number and Collector Density"
subtitle: "Version 3.0"
author: "Jordan Wingenroth"
date: "8/28/2020"
toc: TRUE
output:
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, fig.pos = "!H", out.extra = "")


```

**Objective(s)**

In the interest of documenting our analytic methods, I've decided to make one comprehensive, hopefully lay-readable document including all steps from our raw laboratory data to the model results. I won't be able to include the turbulence analyses in time for our meeting (8/27), but that required Laurel's Matlab script anyways.

## Data Tidying

**CHANGES FROM VERSION 1 (8/26)**

I pared back this section to reduce the page count. Please refer to the original version to see visualizations and more detailed steps and notes.

**Time-decay runs:**

We entered data into Google Sheets as we processed samples in the lab. I downloaded .csv versions of these data in 2019, then made a few modifications as I explored confounders. Today, I added data from runs done in my absence to this dataset. Field names, date syntax, and other conventions weren't kept consistent, so some data wrangling is necessary. I also removed the blank data at the beginning of some files manually.

```{r, message=FALSE, warning=FALSE}

library(tidyverse)

```


```{r}

pumpfiles <- list.files("../data/peristaltic pumps/")
trapfiles <- list.files("../data/sediment traps/")

```

```{r}

pumpdate <- str_sub(pumpfiles,0,6)
trapdate <- str_sub(trapfiles,0,6)

```

In all, five runs are missing sediment trap data, meaning they won't be used in our final, published analyses.

Let's turn our attention to the suspended concentration data (i.e., pump data).

```{r, message=FALSE, warning=FALSE}

pumpdata <- lapply(pumpfiles, function(x) read_csv(paste0("../data/peristaltic pumps/",x)))

names(pumpdata) <- pumpdate

x <- pumpdata

tidypump <- lapply(seq_along(x), function(i) {
  select(x[[i]], 
         loc = Location, 
         ht = Height,
         t = `time series`,
         mvc = contains("(ppm)")) %>%
    filter(t < 21) %>% #filter a few timepoints outside the normal window 
    mutate(t = (t-min(t)+1)*300, #convert from timestep to seconds
           mvc = as.numeric(mvc), 
           date = as.numeric(names(x)[[i]])) %>%
    filter(mvc<80, mvc>8) #outliers were removed based on the residual graph
  }
  )

pump <- bind_rows(tidypump)

```

So now we have a long table of our pump data (time $\times$ concentration) stratified by run (i.e., treatment), height, and upstream/downstream location.

Aside from a few runs where **part** or **all** of the data don't follow the decay pattern, the results look pretty good! If we repeated runs as necessary to replace those with erroneous data, that shouldn't be a problem. Here I'll narrow down our dataset by knocking out runs with clear issues.

```{r}

pump <- pump %>%

#first 3 runs had starting sediment mass of 100g rather than 200g

    filter(date > 181005) %>%
  
#we can't use runs without sediment mass
  
    filter(as.character(date) %in% trapdate)
  
```

So this leaves 20 rows. Let's join this to our run metadata table.

```{r message=FALSE}

metadata <- read_csv("../data/run_metadata.csv")

pump <- left_join(pump, metadata, by = "date")

```

Now then, let's remove the biofouled runs and a couple runs with other issues, which won't be used in assessing our primary hypothesis about Reynolds number and collector density, and we'll see what we're left with.

```{r}

pumpfinal <- pump %>%
  filter(growth_days==0, !date %in% c(190417, 190802, 190926), # leaving out old control and 
         dowel_density != "0232")

```

There are certainly no trends jumping out immediately, which isn't necessarily worrisome since exponential decay is hard to compare by eye. More on that later though. 

For the sediment traps, data import and wrangling follows a similar structure to the pump data.

```{r, message=FALSE, warning=FALSE}

trapdata <- lapply(trapfiles, function(x) read_csv(paste0("../data/sediment traps/",x)))

names(trapdata) <- trapdate

x <- trapdata

tidytrap <- lapply(seq_along(x), function(i) {
  select(x[[i]], station = 1, pre = contains("pre"), post = contains("post"), sed = contains("sed")) %>%
  mutate(date = names(x)[i]) %>%
  mutate_at(vars(pre,post,sed,date), as.numeric)
  }
  )

trap <- bind_rows(tidytrap) %>%
  filter(!is.na(sed))

```

So, across 21 runs, we had 186 samples. We were set up to collect 9 per run, but traps broke, filters slipped and spilled sediment, et cetera. $186/21 =$ `r 186/21`, so our success rate was actually pretty good, although some of those measurements were probably erroneous despite being measured. 

```{r}

trapfinal <- left_join(trap, metadata, by = "date") %>%
  filter(date %in% pumpfinal$date)

```

**Velocity experiment:**

We estimated Reynolds number from a flow velocity experiment in the open channel conducted on Jan 28, 2019. The Vectrino data is in commit `e78357` and the R code for the regression in commit `17c8af` in the esdlflume Github repo (Thanks Candace!).

There's an image in version 1 (and in /pics) but the linear regression is simply $u = 0.00081282 + 0.00195723f$, where $u$ is flow velocity (m/s) and $f$ is pump frequency (Hz). Hence our velocities at 10, 20, and 30 Hz, were `r 0.00081282 + 0.00195723*10`, `r 0.00081282 + 0.00195723*20`, and `r 0.00081282 + 0.00195723*30` respectively.

It should be noted that the back-of-envelope estimate of $v = f / 500$ is very close for our range, probably by design. For the regression, $R^2 > .99$.

**Flume volume experiment:**

The test section is modeled as a rectangular prism, making calculation of volume trivial. However, the water in the flume is in an irregular form. In order to correct for the time water spends flowing outside the test section, where collectors are not acting on particles, we must estimate the total volume of water. We did so using a simple integration of the volumetric flow rate at the drain. We ran an experiment previously in 2018 or 2019, but my physics common sense lapsed: We picked the drain hose up off the ground to take measurements, then stuck it back down in the drain for most of the flow time. This led to an underestimate of about 5-10%. I re-did the analysis. 1st through 4th degree polynomial regression all yield the same volume +/- <0.5% (code in this repo, under /code)

I rounded off to 2.43 $\text m^3$. The test section has been measured as $1.95 \times .6 \times .4 =$ `r 1.95*.6*.4` $\text m^3$. Hence the corrective factor is `r 2.43/0.468`.

## Modelling for ECE%

So at this point, we really only need `pumpfinal`, `trapfinal`, maybe `metadata` for good measure, the frequency-to-velocity regression coefficients, and individual constants about the physical setup such as dowel diameter, flume volume, starting sediment mass, dynamic viscosity etc.

I'll also mention here that our model is based on the general equation: $$k_{tot} = k_s + k_c + k_{bg}$$ where the k's represent total decay rate and portions of it due to settling, collection, and (background) settling in the rest of the flume outside the test section, respectively.

### Uncertainty propogation function: mutate_with_error

Here is a function found on StackExchange.com, which performs error propogation in R using derivatives. Error values should be named "dx", with x being the corresponding variable name.

```{r}

mutate_with_error = function(.data, f) {
  exprs = list(
    # expression to compute new variable values
    deparse(f[[3]]),
    
    # expression to compute new variable errors
    sapply(all.vars(f[[3]]), function(v) {
      dfdp = deparse(D(f[[3]], v))
      sprintf('(d%s*(%s))^2', v, dfdp)
    }) %>%
      paste(collapse='+') %>%
      sprintf('sqrt(%s)', .)
  )
  names(exprs) = c(
    deparse(f[[2]]),
    sprintf('d%s', deparse(f[[2]]))
  )
  
  .data %>%
    # the standard evaluation alternative of mutate()
    mutate_(.dots=exprs)
}

```


### k_tot (total time-decay rate)

We estimate `k_tot` from our pump data by linear regression of `log(mvc)`:

```{r, warning=FALSE, message=FALSE}

library(broom)

r2mod <- list()
r2val <- list()
ktval <- list()

for (k in 1:length(unique(pumpfinal$date))) {
  tempdata <- pumpfinal %>%
    filter(date == unique(pumpfinal$date)[k])
  
  r2mod[[k]] <- lm(data = tempdata, formula = log(mvc) ~ t)
  r2val[[k]] <- summary(r2mod[[k]])$r.squared
  ktval[[k]] <- -coef(r2mod[[k]])[2]
  
}

names(r2mod) <- unique(pumpfinal$date)
names(r2val) <- unique(pumpfinal$date)
names(ktval) <- unique(pumpfinal$date)

r2final <- tibble(unlist(r2val), names(unlist(r2val)), unlist(ktval)) %>% #requires single digit start and stop
  transmute(r2 = .[[1]], 
         kt = .[[3]],
         start = str_sub(.[[2]], 6,6),
         stop = str_sub(.[[2]], 12,12),
         date = as.numeric(str_sub(.[[2]], 14,))) %>%
  left_join(metadata, by = "date")

ktdata <- bind_rows(lapply(r2mod, tidy), .id = "date") %>%
  filter(term == "t")

```


So then, k_tot as a function of our independent vars:

```{r}

fitdata <- ktdata %>%
  mutate(k_t = -estimate, dk_t = std.error, date = as.numeric(date)) %>%
  left_join(metadata, by = "date") %>%
  mutate(ad = as.numeric(dowel_density)*.003175^2*2/1.95, Re = pump_freq/500*.003175/9.509e-7) # DOWEL DENSITY CORRECTION INCLUDED HERE

fitdata %>%
  ggplot(aes(x = Re, y = k_t, ymin = k_t - 1.96*dk_t, ymax = k_t + 1.96*dk_t, color = as.character(round(ad,5)))) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 10)) +
  geom_errorbar(width = 10, position = "dodge") +  
  scale_color_manual(values = c("black","blue","green","red")) +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  labs(y = expression(paste(italic(k[t])," (",s^{-1},")")), x = expression(Re[c]), color = expression(italic(ad))) +
  theme_minimal()
  
```


### k_s (time-decay due to settling)

$$ m_s = \frac{k_s}{k_{tot}}(1-e^{-kT})m_0 $$

Or, expressed for $k_s$, $$k_s = \frac{m_s}{m_0}(\frac{1}{1-e^{-kT}})k_{tot}$$

where $m_s$ is mass settled in the test section (estimated from sediment traps), $m_0$ is starting sediment mass (200 g for this study), and $T$ is the total exposure time of the sediment traps (100 min, i.e., 6000 s, for this study).

```{r}

final <- trapfinal %>%
  group_by(date) %>%
  summarise(m_trap = mean(sed)/1000,
            dm_trap = sd(sed)/1000) %>% #average sediment in trap converted from mg to g
  mutate_with_error(m_s ~ m_trap*1.95*.6/(3.14159*.0127^2)) %>% #*A_TS/a_trap
  left_join(fitdata, by = "date") %>%
  mutate_with_error(c_s ~ m_s/200/(1-exp(-k_t*6000))) %>% #intermediate step to squash error 
  mutate_with_error(k_s ~ c_s*k_t)

final %>%
  ggplot(aes(x = Re, y = k_s, ymin = k_s - dk_s, ymax = k_s + dk_s, color = as.character(round(ad,5)))) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 10)) +
  geom_errorbar(width = 10, position = "dodge") +  
  scale_color_manual(values = c("black","blue","green","red")) +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  labs(y = expression(paste(italic(k[s])," (",s^{-1},")")), x = expression(Re[c]), color = expression(italic(ad))) +
  theme_minimal()
  
```

$k_s$ seems negatively related to both dowel density and flow velocity.

### k_bg (time-decay due to settling in the rest of the flume)

In control runs, $k_c = 0$, so we can estimate the decay rate in the rest of the flume as $k_{bg} = k_{tot}-k_s$

```{r}

bgvals <- final %>%
  filter(dowel_density=="0000") %>%
  mutate_with_error(k_bg ~ k_t - k_s) %>%
  select(pump_freq, k_bg, dk_bg) %>%
  arrange(pump_freq)

bgvals
  
```

To add it to the table:

```{r}

final <- left_join(final, bgvals, by = "pump_freq")

```

### k_c (time-decay due to collection)

So, now that we have our values in the table, k_c is just a subtraction away:

```{r}

final <- final %>%
  mutate_with_error(k_c ~ k_t - k_s - k_bg)

final %>%
  filter(dowel_density != "0000") %>%
  ggplot(aes(x = Re, y = k_c, ymin = k_c - dk_c, ymax = k_c + dk_c, color = as.character(round(ad,5)))) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point(position = position_dodge(width = 10)) +
  geom_errorbar(width = 10, position = "dodge") +  
  scale_color_manual(values = c("blue","green","red")) +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  labs(y = expression(paste(italic(k[c])," (",s^{-1},")")), x = expression(Re[c]), color = expression(italic(ad))) +
  theme_minimal()
```

### eta

```{r}

#temp in flume measured at 22.2C with a calibrated thermometer
#plugged into https://www.engineeringtoolbox.com/water-dynamic-kinematic-viscosity-d_596.html
visc = 9.509e-7 #kinematic viscosity, m2/s

d = .003175 #dowel diameter

final <- final %>%
  mutate(frontal_area = as.numeric(dowel_density)*.003175, 
            u = as.numeric(pump_freq)/500, #velocity 
            Re = u*.003175/visc, #Reynolds #
            k_c, k_s, k_bg) %>%
  mutate(eta = k_c/u/frontal_area,
         deta = dk_c/u/frontal_area) %>%
  mutate(eta = eta * 2.43/(1.95*.4*.6),
         deta = deta * 2.43/(1.95*.4*.6)) # don't forget to correct for time out of test section!

final %>%
  filter(dowel_density != "0000") %>%
  mutate(Refac = factor(Re, labels = c("Re = 66.78", "Re = 133.56", "Re = 200.34"))) %>%
  ggplot(aes(x = ad, y = eta, ymin = eta - deta, ymax = eta + deta, color = as.character(round(ad,5)))) +
  geom_line(lty = "dotted", size = 2, alpha = .3) + 
  geom_point() +
  geom_errorbar() +  
  scale_color_manual(values = c("blue","green","red")) +
  scale_y_continuous(labels = scales::percent) +
  facet_wrap(~Refac) +
  labs(y = "Effective capture efficiency", x = expression(italic(ad)), color = expression(italic(ad))) +
  theme_bw()
```

## Addressing Laurel's comments on the outline

Most of Laurel's comments addressed the structure and written content of the paper. However, several recommended new analyses to include.

### 1: Finding $C$ for each level of $ad$, comparing to Fauria

**COMMENT:**

To better flesh out the role of stem density, you could consider figuring out what $C$ would be for each stem density, using the same functional form of the equation as Fauria. $C$ from our study and Fauria's study could be plotted against dimensionless stem density (i.e., $ad$ or solid volume fraction) to determine the nature of the relationship.

**ADDRESSED:**

Fauria's functional form was $CRe_c^{-1.14}R^{0.65}$. Excluding runs with biofilm, they got $C$ values of 1.53 and 3.18 for stem densities of 7209 (high) and 2724 (low) per meter, respectively.

Let's dig into stem density, frontal area, ad, etc.

Fauria didn't use cylinders but they give dimensions for their synthetic grass: average x-y plane cross-section was 3mm x 1mm, and it seems that they set water height to match stem height roughly. Given that their stems faced randomy in all directions, average *frontal* diameter per stem assuming a 3mm x 1mm rectangular prism (to avoid a multiple integral) would be approximated by, in mm,

$$\frac{\int_0^{\pi/2}[3\sin(\theta)+\cos(\theta)]\, \text{d}\theta}{\pi/2} = \frac{(1)-(-3)}{\pi/2}=\frac{8}{\pi}\,.$$

This, converted to meters, times stems per unit bed area gives frontal area per unit volume:  6.93 m$^{-1}$ (low) and 18.36 m$^{-1}$ (high).

$ad$ is frontal area per unit volume times diameter, making it dimensionless. Since it's unclear which $d_c$ to use for non-cylindrical objects, I'll just use that same frontal $d_c$ again: $8/\pi$. This yields $ad$ values of 0.0177 and 0.0467.

Solid volume fraction ($\phi$) is actually quite easy for rectangular prisms, and would theoretically be equivalent for the stems' actual, more trapezoidal shapes, if Fauria averaged correctly. It's simply $l \times w \times I_c$. This yields $\phi$ values of 0.00817 and 0.02163.

As for our values, cylinders make everything quite easy. We already have stem density and $ad$ in the table, and can calculate frontal area and $\phi$ with ease:

```{r}

final <- final %>%
  mutate(frontal = ad/.003175, phi = ad*pi/4)

final %>%
  select(dowel_density, frontal, ad, phi) %>%
  distinct() %>%
  arrange(ad) %>%
  filter(ad>0)

```

Next, particle ratios: Fauria ran models across a range, but without LISST data, we only know $\eta$ for our particles altogether. Given our constant $d_c$ of 0.3175 cm and our median particle size of 25.2 microns, we'd get a particle-to-stem ratio of 
`r 25.2e-6/.3175e-2`. This raised to the .65 power gives a corrective term of 0.04312975.

OK, so now for the C value models:

```{r}

cmods <- final %>%
  mutate(coeff = 0.04312975*Re^-1.14) %>%
  lmList(eta ~ 0 + coeff|phi, data = .)

```

So then, to put our results and Fauria's side by side:

```{r}

c_over_phi <- cbind(rownames(coef(cmods))[2:4],coef(cmods)[2:4,]) %>%
  as.tibble() %>%
  mutate_all(as.numeric) %>%
  mutate(lab = "ESDL") %>%
  rename(phi = V1, C = V2)

fauria <- tibble(phi = c(.00817,.02163), C = c(3.18,1.53), lab = "Fauria")

c_over_phi <- rbind(c_over_phi, fauria)

c_over_phi %>%
  ggplot(aes(x = phi, y = C, color = lab)) +
  geom_point() +
  theme_bw() +
  labs(title = "Comparison of power-law constant to Fauria et al. data",
       x = expression(phi))
  
lm(data = c_over_phi, formula = log10(C) ~ log10(phi)) %>% summary()

```

### 2: Comparing Fauria model predictions to our observations

**COMMENT:**

If you use the same exponents as Fauria but treat C as a free parameter for each stem density configuration, how do predictions compare to the observations? This would be a plot either of Re vs eta for experimental data (points with error bars) and model predictions (lines) or of predicted vs. observed eta (with error bars for observed).

**ADDRESSED:**

```{r}

# curve data 
fauria_pred <- tibble(Re = rep(seq(66,200,length = 1000),3), 
       C = c(rep(46.46556,1000),rep(10.987587,1000),rep(8.983344,1000)),
       phi = c(rep(0.002257447,1000),rep(0.006496250,1000),rep(0.011774453,1000))) %>%
  mutate(eta_est = Re^-1.14*C*0.04312975, ad = phi)

final %>%
  filter(dowel_density != "0000") %>%
  mutate(Refac = factor(Re, labels = c("Re = 66.78", "Re = 133.56", "Re = 200.34"))) %>%
  ggplot(aes(x = Re, y = eta, ymin = eta - deta, ymax = eta + deta, color = as.character(round(phi,5)))) +
  geom_point() +
  geom_errorbar() +  
  geom_line(data = fauria_pred, aes(y = eta_est, x = Re), inherit.aes = FALSE) +
  scale_color_manual(values = c("blue","green","red")) +
  scale_y_continuous(labels = scales::percent) +
  facet_wrap(~ as.character(round(phi,5))) +
  labs(y = "Effective capture efficiency", x = expression(Re), color = expression(phi)) +
  theme_bw() +
  ggtitle("Comparison of experiments and power-law model based on experiments")

```

### 3: Monte Carlo approach

**COMMENT:**

Each data point has a normal distribution associated with the estimate of eta (defined by the estimate value and the associated standard deviation, i.e., the error bars). For each stem density, you can do Monte Carlo sampling in which you draw a different value of eta from each of the three Re by sampling from the underlying distributions. For each realization of the three eta estimates, you would then do a log transformation, fit a straight line, and save the slope of that line. The result is a distribution of slopes over 1000 or more realizations. If the 95% confidence interval for those slopes includes zero, we cannot say with statistical certainty that there is a negative trend with Re. You would do this for each stem density. Then, you would evaluate whether there is a significant trend in eta with stem density by holding Re constant and doing the same thing for each Re. You could show the results as box plots, where you use some symbol (such as an asterisk) to denote those boxes whose 95% confidence interval does not include zero.

**ADDRESSED:**

Models:

```{r message=FALSE, warning=FALSE}

library(cowplot)

reps = 10000

skeleton_eta <- final %>%
  filter(frontal_area > 0) %>%
  select(phi, Re, eta, deta) %>%
  arrange(phi, Re)

monte <- list()
for (i in 1:nrow(skeleton_eta)) {
  monte[[i]] <- tibble(phi = skeleton_eta$phi[i],
                       Re = skeleton_eta$Re[i],
                       eta = rnorm(reps, mean = skeleton_eta$eta[i], sd = skeleton_eta$deta[i]))
}
lmsphi <- bind_rows(monte) %>%
  mutate(ID = paste0(phi, 1:reps),
         logRe = log(Re)) %>%
  lmList(log(eta) ~ logRe|ID, data = .) %>%
  coefficients()
lmsphi <- tibble(phi = str_sub(rownames(lmsphi),0,6),lmsphi)
lmsphi <- lmsphi %>%
  group_by(phi) %>%
  summarise(avg = mean(logRe, na.rm = TRUE), sd = sd(logRe, na.rm = TRUE)) %>%
  mutate(ymax = avg + 1.96*sd)
lmsphi
  
lmsre <- bind_rows(monte) %>%
  mutate(ID = paste0(Re, 1:reps),
         logphi = log(phi)) %>%
  lmList(log(eta) ~ logphi|ID, data = .) %>%
  coefficients()

lmsre <- tibble(Re = str_sub(rownames(lmsre),0,6),lmsre)

lmsre <- lmsre %>%
  group_by(Re) %>%
  summarise(avg = mean(logphi, na.rm = TRUE), sd = sd(logphi, na.rm = TRUE)) %>%
  mutate(ymax = avg + 1.96*sd)

lmsre

```

Plots:

```{r message=FALSE, warning=FALSE}

p_phi <- lmsphi %>%
  ggplot(aes(x = paste0(100*as.numeric(phi),"%"), y = avg, ymin = avg-1.96*sd, ymax = avg+1.96*sd)) +
  geom_point() +
  geom_errorbar() +
  theme_bw() +
  ylim(-3,1.5) +
  xlab(expression(paste("Solid volume fraction (",phi,")"))) +
  ylab(expression(paste("Effect size: ",beta[log(Re)])))

p_re <- lmsre %>%
  ggplot(aes(x = as.factor(round(as.numeric(Re),0)), y = avg, ymin = avg-1.96*sd, ymax = avg+1.96*sd)) +
  geom_point() +
  geom_errorbar() +
  theme_bw() +
  ylim(-3,1.5) +
  xlab(expression(Re[c])) +
  ylab(expression(paste("Effect size: ",beta[log(phi)])))

plot_grid(p_re, p_phi, labels = c("(a)","(b)"), scale = .9) 

```

### 3 addendum: Safety check on null result (assuming no effect of treatments on k_c)

**ALL DATA IN THIS SECTION IS IMAGINARY!!!!**

Let's see what results we'd get with these same analyses if we detected no effects of Re or $\phi$ on k_c.

```{r}

imaginary_eta <- final %>%
  filter(frontal_area > 0) %>%
  mutate(imagk_c = mean(k_c), 
         imagdk_c = mean(dk_c),
         imageta = imagk_c/u/frontal_area,
         imagdeta = imagdk_c/u/frontal_area) %>%
  mutate(imageta = imageta * 2.43/(1.95*.4*.6),
         imagdeta = imagdeta * 2.43/(1.95*.4*.6)) %>%
  select(phi, Re, eta = imageta, imagdeta)



```

Grouped by stem density:

```{r warning=FALSE}

monte <- list()

for (i in 1:nrow(imaginary_eta)) {
  monte[[i]] <- tibble(phi = imaginary_eta$phi[i],
                       Re = imaginary_eta$Re[i],
                       eta = rnorm(1000, mean = imaginary_eta$eta[i], sd = imaginary_eta$imagdeta[i]))
}

lmsphi <- bind_rows(monte) %>%
  mutate(ID = paste0(phi, 1:1000)) %>%
  lmList(log(eta) ~ Re|ID, data = .) %>%
  coefficients()

lmsphi <- tibble(phi = str_sub(rownames(lmsphi),0,6),lmsphi)


lmsphi <- lmsphi %>%
  group_by(phi) %>%
  summarise(avg = mean(Re, na.rm = TRUE), sd = sd(Re, na.rm = TRUE)) %>%
  mutate(ymax = avg + 1.96*sd)

lmsphi
  
lmsphi %>%
  ggplot(aes(x = phi, y = avg, ymin = avg-1.96*sd, ymax = avg+1.96*sd)) +
  geom_point() +
  geom_errorbar() +
  theme_bw()


```

